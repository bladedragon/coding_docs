# Zookeeper

数据传输的问题：**拜占庭将军问题** 。它意指 **在不可靠信道上试图通过消息传递的方式达到一致性是不可能的**， 所以所有的一致性算法的 **必要前提** 就是安全可靠的消息通道







## 一致性协议



### 2PC(两阶段提交)

> 很多数据库采用两阶段提交完成分布式事务的处理

**第一阶段：**

当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送 `prepare` 请求（其中包括事务内容）告诉参与者你们需要执行事务了，如果能执行我发的事务内容那么就先执行但不提交，执行后请给我回复。

然后参与者收到 `prepare` 消息后，他们会开始执行事务（但不提交），并将 `Undo` 和 `Redo` 信息记入事务日志中，之后参与者就向协调者反馈是否准备好了。

**第二阶段：**

第二阶段主要是协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作，即提交事务或者回滚事务。

比如这个时候 **所有的参与者** 都返回了准备好了的消息，这个时候就进行事务的提交，协调者此时会给所有的参与者发送 **`Commit` 请求** ，当参与者收到 `Commit` 请求的时候会执行前面执行的事务的 **提交操作** ，提交完毕之后将给协调者发送提交成功的响应。

而如果在第一阶段并不是所有参与者都返回了准备好了的消息，那么此时协调者将会给所有参与者发送 **回滚事务的 `rollback` 请求**，参与者收到之后将会 **回滚它在第一阶段所做的事务处理** ，然后再将处理情况返回给协调者，最终协调者收到响应后便给事务发起者返回处理失败的结果。

#### 缺点

只解决了各个事务的原子性，仍然存在：

- **单点故障问题**，如果协调者挂了那么整个系统都处于不可用的状态了。
- **阻塞问题**，即当协调者发送 `prepare` 请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。
- **数据不一致问题**，比如当第二阶段，协调者只发送了一部分的 `commit` 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。

### 3PC（三阶段提交）

1. **CanCommit阶段**：协调者向所有参与者发送 `CanCommit` 请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。

2. **PreCommit阶段**：协调者根据参与者返回的响应来决定是否可以进行下面的 `PreCommit` 操作。如果上面参与者返回的都是 YES，那么协调者将向所有参与者发送 `PreCommit` 预提交请求，**参与者收到预提交请求后，会进行事务的执行操作，并将 `Undo` 和 `Redo` 信息写入事务日志中** ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。

   如果在第一阶段协调者收到了 **任何一个 NO** 的信息，或者 **在一定时间内** 并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送**中断请求（`abort`）**，参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。

3. **DoCommit阶段**：这个阶段其实和 `2PC` 的第二阶段差不多，如果协调者收到了所有参与者在 `PreCommit` 阶段的 YES 响应，那么协调者将会给所有参与者发送 `DoCommit` 请求，**参与者收到 `DoCommit` 请求后则会进行事务的提交工作**，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。

   若协调者在 `PreCommit` 阶段 **收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应** ，那么就会进行中断请求的发送，参与者收到中断请求后则会 **通过上面记录的回滚日志** 来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。



![3PC流程](http://img.francisqiang.top/img/3PC.jpg)

#### 优点

+ 防止同步阻塞：各处存在超时中断，减少阻塞时间
+ 第三阶段没有收到协调者发送的请求，在一定时间内参与者也会进行事务提交，因为通过**第一阶段可以得知所有参与者都能进行事务的进行和提交**，因此进入第三阶段的参与者都会进行事务的提交操作



### Paxos

> **其解决的问题就是在分布式系统中如何就某个值（决议）达成一致** 。

#### 角色

- Proposers：
  - 活动的：提出特定的值被选中
  - 处理客户请求
- Acceptors：
  - 被动的：响应消息给proposers
  - 回应达成共识的投票
  - 保存选中的值
  - 想知道哪个值被选中
- Learner：
  + 
- 每个服务器都包含proposer和acceptor



#### 执行过程

##### prepare 阶段

- `Proposer提案者`：负责提出 `proposal`，每个提案者在提出提案时都会首先获取到一个 **具有全局唯一性的、递增的提案编号N**，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在**第一阶段是只将提案编号发送给所有的表决者**。
- `Acceptor表决者`：每个表决者在 `accept` 某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个**编号最大的提案**，其编号假设为 `maxN`。每个表决者仅会 `accept` 编号大于自己本地 `maxN` 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 `Proposer` 。

##### accept 阶段

+ 当一个提案被 `Proposer` 提出后，如果 `Proposer` 收到了超过半数的 `Acceptor` 的批准（`Proposer` 本身同意），那么此时 `Proposer` 会给所有的 `Acceptor` 发送真正的提案（你可以理解为第一阶段为试探），这个时候 `Proposer` 就会发送提案的内容和提案编号。

+ 表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号 **大于等于** 已经批准过的最大提案编号，那么就 `accept` 该提案（此时执行提案内容但不提交），随后将情况返回给 `Proposer` 。如果不满足则不回应或者返回 NO 。

+ 当 `Proposer` 收到超过半数的 `accept` ，那么它这个时候会向所有的 `acceptor` 发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的 `acceptor` 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要**向未批准的 `acceptor` 发送提案内容和提案编号并让它无条件执行和提交**，而对于前面已经批准过该提案的 `acceptor` 来说 **仅仅需要发送该提案的编号** ，让 `acceptor` 执行提交就行了。
+ 而如果 `Proposer` 如果没有收到超过半数的 `accept` 那么它将会将 **递增** 该 `Proposal` 的编号，然后 **重新进入 `Prepare` 阶段**

##### 缺点

+ 多人提出提案容易出现死循环:每次只能一个人提出提案



### ZAB

> 原子广播，是一种支持**崩溃恢复**的一致性协议
>
> 基于该协议，ZooKeeper实现了一种主从模式的系统架构来保持集群中各个副本之间的数据一致性。

#### 角色

- **Leader** 

  一个ZooKeeper集群同一时间只会有一个实际工作的Leader，它会发起并维护与各Follwer及Observer间的心跳。所有的写操作必须要通过Leader完成再由Leader将写操作广播给其它服务器。

- **Follower** 

  一个ZooKeeper集群可能同时存在多个Follower，它会响应Leader的心跳。Follower可直接处理并返回客户端的读请求，同时会将写请求转发给Leader处理，并且负责在Leader处理写请求时对请求进行投票。

- **Observer** 

  角色与Follower类似，但是无投票权。



#### 消息广播模式

 第一步需要 `Leader` 将写请求 **广播** 出去呀，让 `Leader` 问问 `Followers` 是否同意更新，如果超过半数以上的同意那么就进行 `Follower` 和 `Observer` 的更新（和 `Paxos` 一样）

![消息广播](http://img.francisqiang.top/img/%E6%B6%88%E6%81%AF%E5%B9%BF%E6%92%AD1.jpg)

**Queue的作用：**

**`ZAB` 需要让 `Follower` 和 `Observer` 保证顺序性**，在 `Leader` 这端，它为每个其他的 `zkServer` 准备了一个 **队列** ，采用先进先出的方式发送消息。由于协议是 **通过 `TCP` **来进行网络通信的，保证了消息的发送顺序性，接受顺序性也得到了保证。

除此之外，在 `ZAB` 中还定义了一个 **全局单调递增的事务ID `ZXID`** ，它是一个64位long型，其中高32位表示 `epoch` 年代，低32位表示事务id。`epoch` 是会根据 `Leader` 的变化而变化的，当一个 `Leader` 挂了，新的 `Leader` 上位的时候，年代（`epoch`）就变了。而低32位可以简单理解为递增的事务id。



#### 选举机制

> 基于TCP的FastLeaderElection

**1、myid**

每个ZooKeeper服务器，都需要在数据文件夹下创建一个名为`myid`的文件，该文件包含整个ZooKeeper集群唯一的`ID`（整数）。

例如，某ZooKeeper集群包含三台服务器，`hostname`分别为zoo1、zoo2和zoo3，其`myid`分别为1、2和3，则在配置文件中其ID与`hostname`必须一一对应，如下所示。在该配置文件中，`server`.后面的数据即为`myid`

```javav
server.1=zoo1:2888:3888
server.2=zoo2:2888:3888
server.3=zoo3:2888:3888
```



**2、zxid**

类似于`RDBMS`中的事务`ID`，用于标识一次更新操作的P`roposal ID`。为了保证顺序性，该`zkid`必须单调递增。因此ZooKeeper使用一个`64`位的数来表示，高`32`位是`Leader`的`epoch`，从1开始，每次选出新的`Leader`，`epoch`加一。低`32`位为该`epoch`内的序号，每次`epoch`变化，都将低32位的序号重置。这样保证了`zkid`的全局递增性。

> 用来保证事务的顺序一致性

**3、服务器状态**

每个服务器在进行领导选举时，会发送如下关键信息：

- **logicClock** 每个服务器会维护一个自增的整数，名为logicClock，它表示这是该服务器发起的第多少轮投票
- **state** 当前服务器的状态
- **self_id** 当前服务器的myid
- **self_zxid** 当前服务器上所保存的数据的最大zxid
- **vote_id** 被推举的服务器的myid
- **vote_zxid** 被推举的服务器上所保存的数据的最大zxid

**服务器状态**

- **LOOKING** 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举。
- **FOLLOWING** 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁。
- **LEADING** 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳。
- **OBSERVING** 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票。



##### 选举流程

**自增选举轮次**
ZooKeeper规定所有有效的投票都必须在同一轮次中。每个服务器在开始新一轮投票时，会先对自己维护的`logicClock`进行自增操作。

**初始化选票**
每个服务器在广播自己的选票前，会将自己的投票箱清空。该投票箱记录了所收到的选票。例：服务器2投票给服务器3，服务器3投票给服务器1，则服务器1的投票箱为(2, 3), (3, 1), (1, 1)。票箱中只会记录每一投票者的最后一票，如投票者更新自己的选票，则其它服务器收到该新选票后会在自己票箱中更新该服务器的选票。

**发送初始化选票**
每个服务器最开始都是通过广播把票投给自己。

**接收外部投票**
服务器会尝试从其它服务器获取投票，并记入自己的投票箱内。如果无法获取任何外部投票，则会确认自己是否与集群中其它服务器保持着有效连接。如果是，则再次发送自己的投票；如果否，则马上与之建立连接。

**判断选举轮次**
收到外部投票后，首先会根据投票信息中所包含的`logicClock`来进行不同处理：

- 外部投票的`logicClock`大于自己的`logicClock`。说明该服务器的选举轮次落后于其它服务器的选举轮次，立即清空自己的投票箱并将自己的`logicClock`更新为收到的`logicClock`，然后再对比自己之前的投票与收到的投票以确定是否需要变更自己的投票，最终再次将自己的投票广播出去。
- 外部投票的`logicClock`小于自己的`logicClock`。当前服务器直接忽略该投票，继续处理下一个投票。
- 外部投票的`logickClock`与自己的相等。当时进行选票PK。

**选票PK**
选票PK是基于(`self_id`, `self_zxid`)与(`vote_id`, `vote_zxid`)的对比：

- 外部投票的`logicClock`大于自己的`logicClock`，则将自己的`logicClock`及自己的选票的`logicClock`变更为收到的`logicClock`
- 若`logicClock`一致，则对比二者的`vote_zxid`，若外部投票的`vote_zxid`比较大，则将自己的票中的vote_zxid与vote_myid更新为收到的票中的`vote_zxid`与`vote_myid`并广播出去，另外将收到的票及自己更新后的票放入自己的票箱。如果票箱内已存在(`self_myid`, `self_zxid`)相同的选票，则直接覆盖
- 若二者`vote_zxid`一致，则比较二者的`vote_myid`，若外部投票的`vote_myid`比较大，则将自己的票中的`vote_myid`更新为收到的票中的`vote_myid`并广播出去，另外将收到的票及自己更新后的票放入自己的票箱

**统计选票**

如果已经确定有过半服务器认可了自己的投票（可能是更新后的投票），则终止投票。否则继续接收其它服务器的投票。

**更新服务器状态**

投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为`LEADING`，否则将自己的状态更新为`FOLLOWING`

#### 崩溃恢复机制

**Follower挂了**

如果挂的没超过半数的时候，在 `Leader` 中会维护队列，不用担心后面数据每接受到导致的数据不一致

**Leader挂了**

需要先暂停服务变为 `Looking` 状态然后进行 `Leader` 的重新选举,考虑两种情况：

+ **确保已经被Leader提交的提案最终能够被所有的Follower提交** 

  

+ **跳过那些已经被丢弃的提案** 



### ZAB和Paxos

> 不是特别准确，真实性有待商榷
>
> 个人认为Paxos是分布式一致性的算法，其思想被继承在了ZAB,ZAB是具体的实现，在Paxos的基础上做了一些优化



**联系**

- 两者都存在一个类似于Leader进程的角色，由其负责协调多个Follow进程的运行。
- Leader进程都会等待超过半数的Follower做出正确的反馈后，才会将一个提案进行提交。
- 在ZAB协议中，每个Proposal中都包含了一个epoch值，用来代表当前Leader周期，在Paxos算法中，同样存在这样一个标识，只是名字变成了Ballot。

在Paxos算法中，一个新选举产生的主进程会进行两个阶段的工作。第一阶段被称为读阶段，在这个阶段中，这个新的主进程会通过和所有其他进程进行通信的方式来收集上一个主进程的提案，并将他们提交。第二阶段被称为写阶段，在这个阶段，当前主进程开始提出他自己的提案。

在Paxos算法设计的基础上，ZAB协议额外添加了一个同步阶段。在同步阶段之前，ZAB协议也存在一个和Paxos算法中的读阶段非常类似的过程，称为发现（Discovery）阶段。在同步阶段中，新的Leader会确保存在过半的Follower已经提交了之前Leader周期中的所有事务Proposal。这一同步阶段的引入，能够有效地保证Leader在新的周期中提出事务Proposal之前，所有的进程都已经完成了对之前所有事务Proposal的提交。一旦完成同步阶段后，那么ZAB就会执行和Paxos算法类似的写阶段。

ZAB协议和Paxos算法的本质区别在于，两者的设计目标不太一样。ZAB协议主要用于构建一个高可用的分布式数据主备系统，例如ZooKeeper，而Paxos算法则是用于构建一个分布式的一致性状态机系统。

## 数据模型

**使用了 `znode` 作为数据节点** 。

+ `znode` 是 `zookeeper` 中的最小数据单元，每个 `znode` 上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。

+ 每个 `znode` 都有自己所属的 **节点类型** 和 **节点状态**。

![zk数据模型](http://img.francisqiang.top/img/zk%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B.jpg)



**节点类型**

- 持久节点：一旦创建就一直存在，直到将其删除。
- 持久顺序节点：一个父节点可以为其子节点 **维护一个创建的先后顺序** ，这个顺序体现在 **节点名称** 上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数。
- 临时节点：临时节点的生命周期是与 **客户端会话** 绑定的，**会话消失则节点消失** 。临时节点 **只能做叶子节点** ，不能创建子节点。
- 临时顺序节点：父节点可以创建一个维持了顺序的临时节点(和前面的持久顺序性节点一样)。

**节点状态**

- `czxid`：`Created ZXID`，该数据节点被 **创建** 时的事务ID。
- `mzxid`：`Modified ZXID`，节点 **最后一次被更新时** 的事务ID。
- `ctime`：`Created Time`，该节点被创建的时间。
- `mtime`： `Modified Time`，该节点最后一次被修改的时间。
- `version`：节点的版本号。
- `cversion`：**子节点** 的版本号。
- `aversion`：节点的 `ACL` 版本号。
- `ephemeralOwner`：创建该节点的会话的 `sessionID` ，如果该节点为持久节点，该值为0。
- `dataLength`：节点数据内容的长度。
- `numChildre`：该节点的子节点个数，如果为临时节点为0。
- `pzxid`：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的 **列表** ，不是内容。



### 会话

Zookeeper通过TCP长链接维持会话机制

+ `CONNECTION_LOSS 连接丢失事件` 

+ `SESSION_MOVED 会话转移事件`
+ `SESSION_EXPIRED 会话超时失效事件` 

### ACL

有5种权限：

- `CREATE` ：创建子节点的权限。
- `READ`：获取节点数据和子节点列表的权限。
- `WRITE`：更新节点数据的权限。
- `DELETE`：删除子节点的权限。
- `ADMIN`：设置节点 ACL 的权限。



### 语义保证

- **顺序性**：客户端发起的更新会按发送顺序被应用到 ZooKeeper 上

- **原子性**：更新操作要么成功要么失败，不会出现中间状态

- **单一系统镜像**：一个客户端无论连接到哪一个服务器都能看到完全一样的系统镜像（即完全一样的树形结构）。

  > 注：根据上 ZAB 协议，写操作并不保证更新被所有的 Follower 立即确认，因此通过部分 Follower 读取数据并不能保证读到最新的数据，而部分 Follwer 及 Leader 可读到最新数据。如果一定要保证单一系统镜像，可在读操作前使用 sync 方法。

- **可靠性**：一个更新操作一旦被接受即不会意外丢失，除非被其它更新操作覆盖

- **最终一致性**：写操作最终（而非立即）会对客户端可见

### Watcher机制

> 一个Watch事件是一个**一次性**的触发器，当被设置了Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了Watch的客户端，以便通知它们

`Watcher` 为事件监听器，是 `zk` 非常重要的一个特性，很多功能都依赖于它，它有点类似于订阅的方式，即客户端向服务端 **注册** 指定的 `watcher` ，当服务端符合了 `watcher` 的某些事件或要求则会 **向客户端发送事件通知** ，客户端收到通知后找到自己定义的 `Watcher` 然后 **执行相应的回调方法** 。

![watcher机制](http://img.francisqiang.top/img/watcher%E6%9C%BA%E5%88%B6.jpg)

**为什么watch监听不是永久的？**

- 如果服务端变动频繁，而监听的客户端很多情况下，每次变动都要通知到所有的客户端，这太消耗性能了。
- 一般是客户端执行getData(“/节点A”,true)，如果节点A发生了变更或删除，客户端会得到它的watch事件，但是在之后节点A又发生了变更，而客户端又没有设置watch事件，就不再给客户端发送。

**不支持用持久Watcher的原因很简单，ZK无法保证性能。**

使用watch需要注意的几点

①　Watches通知是一次性的，必须重复注册.

②　发生CONNECTIONLOSS之后，只要在session_timeout之内再次连接上（即不发生SESSIONEXPIRED），那么这个连接注册的watches依然在。

③　节点数据的版本变化会触发NodeDataChanged，注意，这里特意说明了是版本变化。存在这样的情况，只要成功执行了setData()方法，无论内容是否和之前一致，都会触发NodeDataChanged。

④　对某个节点注册了watch，但是节点被删除了，那么注册在这个节点上的watches都会被移除。

⑤　同一个zk客户端对某一个节点注册相同的watch，只会收到一次通知。

⑥　Watcher对象只会保存在客户端，不会传递到服务端。

## 应用场景

### 选主

因为 `Zookeeper` 的强一致性，能够很好地在保证 **在高并发的情况下保证节点创建的全局唯一性** (即无法重复创建同样的节点)。

![选主](http://img.francisqiang.top/img/%E9%80%89%E4%B8%BB.jpg)

我们可以完全 **利用 临时节点、节点状态 和 `watcher` 来实现选主的功能**，临时节点主要用来选举，节点状态和`watcher` 可以用来判断 `master` 的活性和进行重新选举。

### 分布式锁

**获取锁：**

因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，**创建成功的就说明获取到了锁** 。然后没有获取到锁的客户端也像上面选主的非主节点创建一个 `watcher` 进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。

**释放锁**

当客户端挂了，节点也挂了，锁也释放了

**同时实现** **共享锁和独占锁** 

这个时候我规定所有创建节点必须有序，当你是读请求（要获取共享锁）的话，如果 **没有比自己更小的节点，或比自己小的节点都是读请求** ，则可以获取到读锁，然后就可以开始读了。**若比自己小的节点中有写请求** ，则当前客户端无法获取到读锁，只能等待前面的写请求完成。

如果你是写请求（获取独占锁），若 **没有比自己更小的节点** ，则表示当前客户端可以直接获取到写锁，对数据进行修改。若发现 **有比自己更小的节点，无论是读操作还是写操作，当前客户端都无法获取到写锁** ，等待所有前面的操作完成。

这就很好地同时实现了共享锁和独占锁，当然还有优化的地方，比如当一个锁得到释放它会通知所有等待的客户端从而造成 **羊群效应** 。此时你可以通过让等待的节点只监听他们前面的节点。

### 命名服务

如何给一个对象设置ID，大家可能都会想到 `UUID`，但是 `UUID` 最大的问题就在于它太长了

 `zookeeper` 是通过 **树形结构** 来存储数据节点的，那也就是说，对于每个节点的 **全路径**，它必定是唯一的，我们可以使用节点的全路径作为命名方式了。而且更重要的是，路径是我们可以自己定义的，这对于我们对有些有语意的对象的ID设置可以更加便于理解。

### 集群管理和注册中心

我们可以为每条机器创建临时节点，并监控其父节点，如果子节点列表有变动（我们可能创建删除了临时节点），那么我们可以使用在其父节点绑定的 `watcher` 进行状态监控和回调。

![集群管理](http://img.francisqiang.top/img/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86.jpg)



**注册中心**

让 **服务提供者** 在 `zookeeper` 中创建一个临时节点并且将自己的 `ip、port、调用方式` 写入节点，当 **服务消费者** 需要进行调用的时候会 **通过注册中心找到相应的服务的地址列表(IP端口什么的)** ，并缓存到本地(方便以后调用)，当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从地址列表中取一个服务提供者的服务器调用服务。

当服务提供者的某台服务器宕机或下线时，相应的地址会从服务提供者地址列表中移除。同时，注册中心会将新的服务地址列表发送给服务消费者的机器并缓存在消费者本机（当然你可以让消费者进行节点监听，我记得 `Eureka` 会先试错，然后再更新）。



## 操作

[【拓展】ZooKeeper 数据模型和常见命令](https://snailclimb.gitee.io/javaguide/#/docs/system-design/framework/ZooKeeper数据模型和常见命令)





## 常见问题

#### zookeeper是如何选取主leader的？

> 存疑

- 所有节点创建具有相同路径 /app/leader_election/guid_ 的顺序、临时节点。
- ZooKeeper集合将附加10位序列号到路径，创建的znode将是 /app/leader_election/guid_0000000001，/app/leader_election/guid_0000000002等。
- 对于给定的实例，在znode中创建最小数字的节点成为leader，而所有其他节点是follower。
- 每个follower节点监视下一个具有最小数字的znode。例如，创建znode/app/leader_election/guid_0000000008的节点将监视znode/app/leader_election/guid_0000000007，创建znode/app/leader_election/guid_0000000007的节点将监视znode/app/leader_election/guid_0000000006。
- 如果leader关闭，则其相应的znode/app/leader_electionN会被删除。
- 下一个在线follower节点将通过监视器获得关于leader移除的通知。
- 下一个在线follower节点将检查是否存在其他具有最小数字的znode。如果没有，那么它将承担leader的角色。否则，它找到的创建具有最小数字的znode的节点将作为leader。
- 类似地，所有其他follower节点选举创建具有最小数字的znode的节点作为leader。