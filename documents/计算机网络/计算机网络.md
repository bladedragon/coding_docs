​                                                                                                                                                                                                                                                                                                          	

# 计算机网络

## IP层



![image-20200318120257044](documents/计算机网络/image-20200318120257044.png)

TTL的作用：

防止数据包在网络中无限制的传输，路由器每转发一次，跳数减1，跳数为0时候丢弃数据包，最大长度255







## 传输层



**MSS和MTU分片**

**MSS:** 是Maximum Segement Size缩写，表示TCP报文中data部分的最大长度，是TCP协议在OSI五层网络模型中传输层对一次可以发送的最大数据的限制。

**MTU:** 最大传输单元是Maxitum Transmission Unit的简写，是OSI五层网络模型中链路层(datalink layer)对一次可以发送的最大数据的限制。

当需要传输的数据大于MSS或者MTU时，数据会被拆分成多个包进行传输。由于MSS是根据MTU计算出来的，**因此当发送的数据满足MSS时，必然满足MTU。**

**MSS长度=MTU长度-IP Header-TCP Header** 



### 头部

![image-20200225103702516](documents/计算机网络/image-20200225103702516.png)

**紧急URG (URGent)**

当URG = 1时，表明紧急指针字段有效。它告诉系统此报文段中有紧急数据，应尽快传送（相当于高优先级的数据），而不要按原来的排队顺序来传送。例如，已经发送了很长的一个程序要在远地的主机上运行。但后来发现了一些问题，需要取消该程序的运行。因此用户从键盘发出中断命令（Control + C)。如果不使用紧急数据，那么这两个字符将存储在接收TCP的缓存末尾。只有在所有的数据被处理完毕后这两个字符才被交付接收方的应用进程。这样做就浪费了许多时间。



> **还有就是在我们使用Redis做缓存的时候，都需要将放入Redis的数据序列化才可以，原因就是Redis底层就是实现的TCP协议。**



### 三次握手和四次挥手

#### 为什么seq随机产生

如果TCP在建立连接时每次都选择相同的、固定的初始序号，那么设想以下的情况：  

 (1)假定主机A和B频繁地建立连接，传送一些TCP报文段后，再释放连接，然后又不断地建立新的连接、传送报文段和释放连接。   

(2)假定每一次建立连接时，主机A都选择相同的、固定的初始序号，例如，选择1。   

(3)假定主机A发送出的某些TCP报文段在网络中会滞留较长的时间，以致造成主机A超时重传这些TCP报文段。   

(4)假定有一些在网络中滞留时间较长的TCP报文段最后终于到达了主机B，但这时传送该报文段的那个连接早已释放了．而在到达主机B时的TCP连接是一条新的TCP连接。   这样，工作在新的TCP连接下的主机B就有可能会接受在旧的连接传送的、已经没有意义的、过时的TCP报文段(因为这个TCP报文段的序号有可能正好处在现在新的连接所使用的序号范围之中)。结果产生错误。   因此，必须使得迟到的TCP报文段的序号不处在新的连接中所使用的序号范围之中。   这样，TCP在建立新的连接时所选择的初始序号一定要和前面的一些连接所使用过的序号不一样。因此，不同的TCP连接不能使用相同的初始序号。



![image-20200224180713951](documents/计算机网络/image-20200224180713951.png)

SYN报文段（即SYN =1的报文段）不能携带数据，但要消耗掉一个序号

ACK报文段可以携带数据。但如果不携带数据则不消耗序号



![image-20200224180722226](documents/计算机网络/image-20200224180722226.png)

FIN报文段即使不携带数据，它也消耗掉一个序号。

#### TIME_WAIT

客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL

- 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文，A 等待一段时间就是为了处理这种情况的发生。
- 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。
- **主动关闭连接的一方就会进入TIME_WAIT状态**。客户端主动关闭连接时，会发送最后一个ack后，然后会进入TIME_WAIT状态，再停留2个MSL时间(后有MSL的解释)，进入CLOSED状态。最终的ACK是由主动关闭连接的一端（后面统称A端）发出的，如果这个ACK丢失，对方（后面统称B端）将重发出最终的FIN，因此A端必须维护状态信息（TIME_WAIT）允许它重发最终的ACK。如果A端不维持TIME_WAIT状态，而是处于CLOSED 状态，那么A端将响应RST分节，B端收到后将此分节解释成一个错误。因而，要实现TCP全双工连接的正常终止，必须处理终止过程中四个分节任何一个分节的丢失情况，主动关闭连接的A端必须维持TIME_WAIT状态 。
- 大多数服务器端一般执行被动关闭，服务器不会进入TIME_WAIT状态。
- 当在服务器端关闭某个服务再重新启动时，服务器是会进入TIME_WAIT状态的。
- 由于网络原因，ACK可能会发送失败，那么这个时候，被动一方会主动重新发送一次FIN，这个时候如果主动方在TIME_WAIT状态，则还会再发送一次ACK，从而保证可靠性。那么从这个解释来说，2MSL的时长设定是可以理解的，**MSL是报文最大生存时间**，如果重新发送，一个FIN＋一个ACK，再加上不定期的延迟时间，大致是在2MSL的范围。

#### 非正常连接终止·

1. 三次握手完成后，在服务器`accept`前，客户发送`RST`，会导致`accpet`出错，错误如何处理依赖于不同的实现
2. 服务器和客户建立连接后，若服务器进程终止，则服务器发送`FIN`到客户。·
3. 服务器和客户建立连接后，若服务器主机崩溃，有两种可能：
   1. 服务器不重启，客户继续工作，就会发现对方没有回应(`ETIMEOUT`)，路由器聪明的话，则是目的地不可达(`EHOSTUNREACH`)。
   2. 服务器重启后，客户继续工作，然而服务器已丢失客户信息，收到客户数据后响应`RST`。
4. 服务器和客户建立连接后，若服务器关机，`init`进程会给所有进程发送`SIGTERM`信号预警，然后发送`SIGKILL`关闭所有其他进程，这样和2情况没什么差别。



**链路异常后发生的操作**

A侧在超时退出之后一般会发送一个`RST`包用于告知对端重置链路，并给应用层一个异常的状态信息，视乎同步IO与异步IO的差异，这个异常获知的时机会有所不同。

B侧重启之后，因为不存有之前A-B之间建立链路相关的信息，这时候收到任何A侧来的数据都会以`RST`作为响应，以告知A侧链路发生异常

**RST的设计用意在于链路发生意料之外的故障时告知链路上的各方释放资源（一般指的是NAT网关与收发两端）;FIN的设计是用于在链路正常情况下的正常单向终止与结束。二者不可混淆。**



### 底层原理：ARQ协议

#### 超时重传ARQ

确认丢失

确认迟到

#### 连续ARQ（滑动窗口的原理）

>为了提高传输效率，发送方可以不使用低效率的停止等待协议，而是采用流水线传输
>
>流水线传输就是发送方可连续发送多个分组，不必每发完一个分组就停顿下来等待对方的确认。
>
>这样可使信道上一直有数据不间断地在传送

出现差错：

部分分组出现差错：

+ 接收方发送出现差错前的分组确认
+ 发送方回退窗口重发分组



**RTO**：从上一次发送数据，因为长期没有收到ACK响应，到下一次重发之间的时间。就是重传间隔。

+ 通常每次重传RTO是前一次重传间隔的两倍，计量单位通常是RTT。例：1RTT，2RTT，4RTT，8RTT……
+ 重传次数到达上限之后停止重传。

**RTT**：数据从发送到接收到对方响应之间的时间间隔，即数据报在网络中一个往返用时。大小不稳定。

### 滑动窗口机制

+ TCP的窗口滑动技术通过动态改变窗口的大小来调节两台主机之间数据传输。每个TCP/IP主机支持全双工数据传输，因此TCP有两个滑动窗口，一个用于接收数据，一个用于发送数据。**接收方设备要求窗口大小为0时，表明接收方已经接收了全部数据，或者接收方应用程序没有时间读取数据，要求暂停发送。**
+ TCP在传送数据时，**第一次接受方窗口大小是由链路带宽决定的** ，但是接收方在接收到的数据后，返回ack确认报文，同时也告诉了发送方自己的窗口大小，**此时发送方第二次发送数据时，会改变自己的窗口大小和接收方一致**。
+ 当窗口过大时，会导致不必要的数据来拥塞我们的链路，但是窗口太小时，会造成很大的延时，比如为1时，发送方每发送一个数据，接收方就会返回一个ack报文，在发送方未接收到接收方的确认报文ack之前不会进行下一次发送。（当链路变好了或者变差了这个窗口还会发生变化，并不是第一次协商好了以后就永远不变了。还要收到网络拥塞的影响）
+ 接受窗口必须具有**累计确认**的功能，但是延迟确认不能太长

![image-20200224163321199](documents/计算机网络/image-20200224163321199.png)

![image-20200224163533570](documents/计算机网络/image-20200224163533570.png)

**发送窗口后沿变化情况**

+ 不动：没有收到新的确认

+ 前移：收到新得确认

**发送窗口前沿变化情况：**

+ 不动：没有收到新的确认，同时对方的通知窗口大小没有改变；或者收到新的确认但是对方通知窗口大小变小（**窗口最大值取决于对方的通知窗口大小**）
+ 后移：TCP标准不赞成
+ 正常前移



出现差错情况

+ 发送端发送窗口用完，但是已发送的都没有收到确认：超时计时器后重传这些数据
+ 出现未按序到达的数据，接收窗口等待发送窗口发送缺少的数据，再按需上交到上层应用



#### **窗口和缓存**

发送缓存暂时存放：

+ 发送应用程序传输给发送方TCP准备发送的数据
+ TCP已发送但尚未收到确认的数据

**发送窗口是发送缓存的一部分**

接收缓存用来暂时存放：

+ 按序到达，但尚未被读取的数据
+ 未按序到达的数据

**有差错的分组就要被丢弃**



#### 选择确认SACK 	

收到的报文无差错但是未按序到达，发送端不重传所有数据，只传送缺少数据

**需要四个指针确认边界信息**

![image-20200224170127831](documents/计算机网络/image-20200224170127831.png)

#### 流量控制

+ 接收端向发送端发送`rwnd`的大小·

+ TCP每个连接都维护一个**持续计时器**，只要一方收到另一方的零窗口通知，就启动持续计时器。计时时间到期就发送**零窗口的探测报文段**。一方必须重新设置窗口大小，不然，重新设置计时器。防止出现死锁现象



应用将数据发送到发送缓存后，TCP有多种机制控制**TCP报文段的发送时机**

1. TCP维护一个变量等于**最大报文长度**`MSS`，只要缓存中存放的数据到达`MSS`字节，就组装成TCP报文发送出去。
2. 发送方应用进程指明要求发送报文段，即`push`操作
3. 发送方计时器期限到期，将缓存数据装入报文段发送



**Nagle算法 **

> **Nagle算法就是为了尽可能发送大块数据，避免网络中充斥着许多小数据块。**

若发送应用进程把要发送的数据逐个字节地送到TCP的发送缓存，则发送方就把第一个数据字节先发送出去，把后面到达的数据字节都缓存起来。

当发送方收到对第一个数据字符的确认后，再把发送缓存中的所有数据组装成一个报文段发送出去，同时继续对随后到达的数据进行缓存。

只有在收到对前一个报文段的确认后才继续发送下一个报文段。

当数据到达较快而网络速率较慢时，用这样的方法可明显地减少所用的网络带宽。

Nagle算法还规定，当到达的数据己达到发送窗口大小的一半或己达到报文段的最大长度时，就立即发送一个报文段。这样做，就可以有效地提高网络的吞吐量。

**Nagle算法的规则：**

1. 如果SO_SNDBUF(发送缓冲区）中的数据长度达到MSS，则允许发送；
2. 如果该SO_SNDBUF中含有FIN，表示请求关闭连接，则先将SO_SNDBUF中的剩余数据发送，再关闭；
3. 设置了TCP_NODELAY=true选项，则允许发送。TCP_NODELAY是取消TCP的确认延迟机制，相当于禁用了Nagle 算法。
4. 未设置TCP_CORK选项时，若所有发出去的小数据包（包长度小于MSS）均被确认，则允许发送;
5. 上述条件都未满足，但发生了超时（一般为200ms），则立即发送。





### 拥塞机制

在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞

![image-20200224173621713](documents/计算机网络/image-20200224173621713.png)



**闭环控制**

基于反馈环路，检测拥塞发生在何时何地，并采取有效行动

**开环控制**

设计时考虑拥塞因素

#### 控制方法

TCP的拥塞控制采用了四种算法，即 **慢开始** 、 **拥塞避免** 、**快重传** 和 **快恢复**。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。

 

**发送方维持一个拥塞窗口cwnd，发送方让发送窗口等于拥塞窗口**

##### **慢开始**

旧的规定是这样的：在刚刚开始发送报文段时，先把初始拥塞窗口 cwnd设置为1至2个发送方的最大报文段`SMSS (Sender Maximum Segment Size)`的数值，但新的RFC 5681把初始拥塞窗口 `cwnd`设置为不超过`2至4个SMSS`的数值。具体的规定如下：
若 SMSS>2190字节，
则设置初始拥塞窗口 cwnd = 2x SMSS字节，且不得超过2个报文段。
若（SMSS > 1095字节）且（SMSS <2190字节），
则设置初始拥塞窗口 cwnd = 3 x SMSS字节，且不得超过3个报文段。
若 SMSS S 1095字节，
则设置初始拥塞窗口 cwnd = 4x SMSS字节，且不得超过4个报文段。
可见这个规定就是限制初始拥塞窗口的字节数。

慢开始规定，在每收到一个对新的报文段的确认后，可以把拥塞窗口增加最多一个
`SMSS`的数值。更具体些，就是
`拥塞窗口 cwnd每次的增加量=min (M ,SMSS)`

![image-20200224174726541](documents/计算机网络/image-20200224174726541.png)



为了防止拥塞窗口 `cwnd`增长过大引起网络拥塞，还需要设置一个**慢开始门限**`ssthresh`
状态变量（如何设置ssthresh，后面还要讲)。慢开始门限ssthresh的用法如下：
当cwnd < ssthresh时，使用上述的慢开始算法。
当cwnd > ssthresh时，停止使用慢开始算法而改用拥塞避免算法。
当cwnd = ssthresh时，既可使用慢开始算法，也可使用拥塞避免算法。

##### 拥塞避免

> 拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送放的cwnd加1.

![image-20200224175045225](documents/计算机网络/image-20200224175045225.png)



当拥塞窗口 cwnd = 24时，网络出现了超时（图中的点❷），发送方判断为网络拥塞。
于是调整门限值`ssthresh = cwnd/2= 12`，同时设置拥塞窗口 cwnd = 1,进入慢开始阶段。

##### 快重传

有时，个别报文段会在网络中丢失，但实际上网络并未发生拥塞。如果发送方迟迟收
不到确认，就会产生超时，就会误认为网络发生了拥塞。这就导致发送方错误地启动慢开
始，把拥塞窗口 cwnd又设置为1,因而降低了传输效率。

> 采用快重传算法可以让发送方尽早知道发生了个别报文段的丟失

快重传算法首先要求接收方不要等待自己发送数据时才进行**捎带确认**，而是要立即发送确认，即使收到了失序的报文段也要立即发出对己收到的报文段的重复确认

![image-20200224175536862](documents/计算机网络/image-20200224175536862.png)

因此，在图5-25中的点O,发送方知道现在只是丢失了个别的报文段。于是不启动慢
开始，而是执行**快恢复算法**。

##### **快恢复**

这时，发送方调整门限值ssthreSh = Cwnd/2 = 8,同时设置拥塞窗口 cwnd = ssthresh = 8 (见图5-25中的点❺），并开始执行拥塞避免算法



![image-20200224175929969](documents/计算机网络/image-20200224175929969.png)



#### 综合考虑

在这一节的开始我们就假定了接收方总是有足够大的缓存空间，因而发送窗口的大小
由网络的拥塞程度来决定。

但实际上接收方的缓存空间总是有限的。接收方根据自己的接收能力设定了**接收方窗口 `rwnd`**,并把这个窗口值写入TCP首部中的窗口字段，传送给发送方。因此，接收方窗口又称为**通知窗口**`(advertised window)`。**因此，从接收方对发送方的流量控制的角度考虑，发送方的发送窗口一定不能超过对方给出的接收方窗口值`rwnd`。**



> **发送方窗口的上限值=Min [rwnd, cwnd]**



### AQM·

网络层的策略对TCP拥塞控制影响最大的就是路由器的分组丢弃策略。在最简单的情况下，路由器的队列通常都是按照“先进先出” `FIFO (First In First Out)`的规则处理到来的分组。由于队列长度总是有限的，因此当队列己满时，以后再到达的所有分组（如果能够继续排队，这些分组都将排在队列的尾部）将都被丢弃。这就叫做**尾部丟弃策略(tail-drop policy)。**

路由器的尾部丢弃往往会导致一连串分组的丢失，这就使发送方出现**超时重传**，使TCP进入拥塞控制的**慢开始状态**，结果使TCP连接的发送方突然把数据的发送速率降低到很小的数值。

更为严重的是，在网络中通常有很多的TCP连接（它们有不同的源点和终点），这些连接中的报文段通常是复用在网络层的IP数据报中传送。在这种情况下，若发生了路由器中的尾部丢弃，就可能会同时影响到很多条TCP连接，结果使这许多TCP连接在同一时间突然都进入到慢开始状态。这在TCP的术语中称为**全局同步(global syncronization)**。**全局同步使得全网的通信量突然下降了很多，而在网络恢复正常后，其通信量又突然增大很多。**





#### 随机早期检测RED

> AQM的实现方法之一

实现RED时需要使路由器维持两个参数，即**队列长度最小门限**和**最大门限**。当每一个
分组到达时，RED就按照规定的算法先计算当前的平均队列长度。

1. 若平均队列长度小于最小门限，则把**新到达的分组放入队列进行排队。**
2. 若平均队列长度超过最大门限，则把**新到达的分组丢弃。**
3. 若平均队列长度在最小门限和最大门限之间，则**按照某一丢弃概率把新到达的分**
   **组丢弃（这就体现了丢弃分组的随机性)。**





## DNS

### 域名解析过程

　　在浏览器输入一串域名要访问某网站的时候，浏览器帮我们做了如下事情（以Chrome浏览器和windows系统为例）：

1. Chrome**浏览器**首先检查自己本地是缓存是否有对应的域名，有则直接使用。【查看Chrome浏览器dns缓存地址：[chrome://net-internals/#dns](chrome://net-internals/#dns)】
2. 如果浏览器缓存中没有，则查询**系统DNS缓存中的域名表**，有则直接使用。【windows查看域名表的命令：**ipconfig /displaydns**】
3. 系统缓存中还是没有，则检查**hosts文件**中的映射表。【windows中hosts文件路径：C:\Windows\System32\drivers\etc】
4. 路由器查询DNS缓存
5. 本地实在找不到，则向DNS域名服务器发起请求查询。【DNS服务器IP是本地配置的首选服务器，一般常用的有114.114.114.114（电信运营商提供）和8.8.8.8（Google提供）】

- - - DNS服务器首先查找自身的缓存，有对应的域名ip则返回结果
    - 如果缓存中查找不到，DNS服务器则发起迭代DNS请求，首先向根域服务器发起请求查询，假如本次请求的是www.baidu.com,根域服务器发现这是一个com的顶级域名，就把com域的ip地址返回给DNS服务器
    - DNS服务器向com域ip地址发起请求，查询该域名的ip，此时该服务器返回了baidu.com的DNS地址。
    - 最后DNS服务器又向baidu.com的DNS地址发起查询请求，最后找到了完整的ip路径返回给DNS服务器，DNS再把ip信息返回给windows内核，内核再返回给浏览器，于是浏览器就知道该域名对应的ip地址了，可以开始进一步请求了。



### **DNS请求过程**

DNS在进行区域传输的时候使用`TCP`协议，其它时候则使用`UDP`协议； DNS的规范规定了2种类型的DNS服务器，一个叫**主DNS服务器**，一个叫**辅助DNS服务器**。·

**在一个区中主DNS服务器从自己本机的数据文件中读取该区的DNS数据信息，而辅助DNS服务器则从区的主DNS服务器中读取该区的DNS数据信息**。当一个辅助DNS服务器启动时，它需要与主DNS服务器通信，并加载数据信息，这就叫做**区传送（zone transfer）**。

---



 **为什么既使用TCP又使用UDP？** 

首先了解一下TCP与UDP传送字节的长度限制： UDP报文的最大长度为`512`字节，而TCP则允许报文长度`超过512字节`。当DNS查询超过512字节时，**协议的TC标志出现删除标志**，这时则使用TCP发送。通常传统的UDP报文一般不会大于512字节。 

区域传送时使用TCP，主要有一下两点考虑： 

1. 辅域名服务器会定时（一般时3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，则会执行一次区域传送，进行数据同步。区域传送将使用TCP而不是UDP，**因为数据同步传送的数据量比一个请求和应答的数据量要多得多。**

2. TCP是一种可靠的连接，保证了数据的准确性。 

域名解析时使用UDP协议： 

客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过TCP三次握手，**这样DNS服务器负载更低，响应更快**。虽然从理论上说，客户端也可以指定向DNS服务器查询的时候使用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。 

---



 UDP 与 TCP 的主要区别在于 UDP 不一定提供可靠的数据传输。事实上，该协议不能保证数据准确无误地到达目的地。UDP 在许多方面非常有效。当某个程序的目标是尽快地传输尽可能多的信息时（其中任意给定数据的重要性相对较低），可使用 UDP。ICQ 短消息使用 UDP 协议发送消息。 许多程序将使用单独的TCP连接和单独的UDP连接。重要的状态信息随可靠的TCP连接发送，而主数据流通过UDP发送。 

 TCP的目的是提供可靠的数据传输，并在相互进行通信的设备或服务之间保持一个虚拟连接。TCP在数据包接收无序、丢失或在交付期间被破坏时，负责数据恢复。**它通过为其发送的每个数据包提供一个序号来完成此恢复**。记住，较低的网络层会将每个数据包视为一个独立的单元，因此，数据包可以沿完全不同的路径发送，即使它们都是同一消息的组成部分。这种路由与网络层处理分段和重新组装数据包的方式非常相似，只是级别更高而已。 **为确保正确地接收数据，TCP要求在目标计算机成功收到数据时发回一个确认（即 ACK）**。如果在某个时限内未收到相应的 ACK，将重新传送数据包。如果网络拥塞，这种重新传送将导致发送的数据包重复。但是，接收计算机可使用数据包的序号来确定它是否为重复数据包，并在必要时丢弃它。 

---



**TCP与UDP的选择** 

如果比较UDP包和TCP包的结构，很明显UDP包不具备TCP包复杂的可靠性与控制机制。与TCP协议相同，UDP的源端口数和目的端口数也都支持一台主机上的多个应用。

一个16位的UDP包包含了一个字节长的头部和数据的长度，校验码域使其可以进行整体校验。（许多应用只支持UDP，如：多媒体数据流，不产生任何额外的数据，即使知道有破坏的包也不进行重发。） 

很明显，当数据传输的性能必须让位于数据传输的完整性、可控制性和可靠性时，TCP协议是当然的选择。当强调传输性能而不是传输的完整性时，如：音频和多媒体应用，UDP是最好的选择。在数据传输时间很短，以至于此前的连接过程成为整个流量主体的情况下，UDP也是一个好的选择，如：DNS交换。把SNMP建立在UDP上的部分原因是设计者认为当发生网络阻塞时，UDP较低的开销使其有更好的机会去传送管理数据。



### TCP**长连接和短链接**

**TCP短连接**

模拟一下TCP短连接的情况：client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次请求就完成了。这时候双方任意都可以发起close操作，不过一般都是client先发起close操作。上述可知，短连接一般只会在 client/server间传递一次请求操作。

短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。

**TCP长连接**

我们再模拟一下长连接的情况：client向server发起连接，server接受client连接，双方建立连接，client与server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。

TCP的保活功能主要为服务器应用提供。如果客户端已经消失而连接未断开，则会使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，此时服务器将永远等待客户端的数据。保活功能就是试图在服务端器端检测到这种半开放的连接。

如果一个给定的连接在**两小时**内没有任何动作，服务器就向客户发送一个**探测报文段**，根据客户端主机响应探测4个客户端状态：

- 客户主机依然正常运行，且服务器可达。此时客户的TCP响应正常，服务器将保活定时器复位。
- 客户主机已经崩溃，并且关闭或者正在重新启动。上述情况下客户端都不能响应TCP。**服务端将无法收到客户端对探测的响应。服务器总共发送10个这样的探测，每个间隔75秒**。若服务器没有收到任何一个响应，它就认为客户端已经关闭并终止连接。
- 客户端崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，**这个响应是一个复位**，使得服务器终止这个连接。
- 客户机正常运行，但是**服务器不可达**。这种情况与第二种状态类似。



**什么时候用长连接，短连接？** 　　

**长连接**多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 

而像WEB网站的http服务一般都用**短链接**，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。

## HTTP

[HTTP请求过程](https://www.cnblogs.com/caijh/p/7661402.html)

[状态码大全](https://blog.csdn.net/qq_39376481/article/details/89219704)

HTTP无状态？

> HTTP协议是无状态的（stateless），指的是协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。也就是说，打开一个服务器上的网页和上一次打开这个服务器上的网页之间没有任何联系。HTTP是一个无状态的面向连接的协议，无状态不代表HTTP不能保持TCP连接，更不能代表HTTP使用的是UDP协议（无连接）。  缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 





[HTTP1.0、HTTP1.1 和 HTTP2.0 的区别](https://mp.weixin.qq.com/s/GICbiyJpINrHZ41u_4zT-A?)

### **HTTP1.0和HTTP1.1的一些区别**

HTTP1.0最早在网页中使用是在1996年，那个时候只是使用一些较为简单的网页上和网络请求上，而HTTP1.1则在1999年才开始广泛应用于现在的各大浏览器网络请求中，同时HTTP1.1也是当前使用最为广泛的HTTP协议。 主要区别主要体现在：

1. **缓存处理**，在HTTP1.0中主要使用header里的`If-Modified-Since`,`Expires`来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如`Entity tag`，`If-Unmodified-Since`, `If-Match`, `If-None-Match`等更多可供选择的缓存头来控制缓存策略。
2. **带宽优化及网络连接的使用**，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了`range`头域，它允许只请求资源的某个部分，即返回码是`206`（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。
3. **错误通知的管理**，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。
4. **Host头处理**，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（`hostname`）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（`Multi-homed Web Servers`），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。
5. **长连接**，HTTP 1.1支持长连接（`PersistentConnection`）和请求的流水线（`Pipelining`）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。

### **HTTP2.0和HTTP1.X相比的新特性**

- **新的二进制格式**（Binary Format），HTTP1.x的解析是`基于文本`。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用`二进制格式`，实现方便且健壮。
- **多路复用**（MultiPlexing），即连接共享，即每一个request都是是用作连接共享机制的。`一个request对应一个id`，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。
  多路复用代替了HTTP1.x的**序列和阻塞机制**，所有的相同域名请求都通过同一个TCP连接并发完成。在HTTP1.x中，并发多个请求需要多个TCP连接，浏览器为了控制资源会有6-8个TCP连接都限制。
  HTTP2中
  - **同域名下所有通信都在单个连接上完成**，消除了因多个 TCP 连接而带来的延时和内存消耗。
  - **单个连接上可以并行交错的请求和响应**，之间互不干扰

- **header压缩**，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用`encoder`来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。
- **服务端推送**（server push），同SPDY一样，HTTP2.0也具有`server push`功能。



**HTTP2.0的多路复用和HTTP1.X中的长连接复用有什么区别？**

- HTTP/1.* 一次请求-响应，建立一个连接，用完关闭；每一个请求都要建立一个连接；
- HTTP/1.1 Pipeling解决方式为，若干个请求排队串行化单线程处理，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时等，后续请求只能被阻塞，毫无办法，也就是人们常说的线头阻塞；
- HTTP/2多个请求可同时在一个连接上并行执行。某个请求任务耗时严重，不会影响到其它连接的正常执行；

![img](http://mmbiz.qpic.cn/mmbiz_png/cmOLumrNib1cfBOtIMQ6JfSibJdd6QkQriba5ygCTOOjIQH4wvoJS2iaFBseyEAUfvpJQThHmTjuGuaSspUo8xppiaA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**服务器推送到底是什么？**
服务端推送能把客户端所需要的资源伴随着index.html一起发送到客户端，省去了客户端重复请求的步骤。正因为没有发起请求，建立连接等操作，所以静态资源通过服务端推送的方式可以极大地提升速度。



>http/1.0：如需要发送多个请求必须创建多个 TCP 连接，并且浏览器对于单域名请求有数量限制（一般6个），其连接无法被复用

> http/1.1：引入流水线（Pipelining）技术，但先天 FIFO（先进先出）机制导致当前请求的执行依赖于上一个请求执行的完成，容易引起报头阻塞，并没有从根本上解决问题

> http/2：重新定义底层 http 语义映射，允许同一个连接上使用请求和响应双向数据流。同一域名只需占用一个 TCP 连接，通过数据流（Stream）以帧为基本协议单位，从根本上解决了问题，避免了因频繁创建连接产生的延迟，减少了内存消耗，提升了使用性能

### QUIC协议

当其中一个数据包遇到问题，TCP连接需要等待找个包完成重传之后才能继续进行，虽然HTTP2.0通过多个stream，使得逻辑上一个tcp连接上的并行内容，进行多路数据的传输，然而这中间没有关联的数据，一前一后，前面stream2的帧没有收到，后面stream1的帧也会因此堵塞

**于是google的 QUIC协议从TCP切换到UDP**

- 机制一：自定义连接机制
  一条tcp连接是由四元组标识的，分别是源ip、源端口、目的端口，一旦一个元素发生变化时，就会断开重连，重新连接。在次进行三次握手，导致一定的延时

在TCP是没有办法的，但是基于UDP，就可以在QUIC自己的逻辑里面维护连接的机制，不再以四元组标识，而是以**一个64位的随机数作为ID来标识，而且UDP是无连接的，所以当ip或者端口变化的时候，只要ID不变，就不需要重新建立连接**

- 机制二：自定义重传机制
  tcp为了保证可靠性，通过使用序号和应答机制，来解决顺序问题和丢包问题

任何一个序号的包发过去，都要在一定的时间内得到应答，否则一旦超时，就会重发这个序号的包，通过自适应重传算法（通过采样往返时间RTT不断调整）

但是，在TCP里面超时的采样存在不准确的问题。例如发送一个包，序号100，发现没有返回，于是在发送一个100，过一阵返回ACK101.客户端收到了，但是往返的时间是多少，没法计算。是ACK到达的时候减去第一还是第二。

**QUIC也有个序列号，是递增的，任何序列号的包只发送一次，下次就要加1，那样就计算可以准确了**

但是有一个问题，就是怎么知道包100和包101发送的是同样的内容呢？quic定义了一个`offset`概念。QUIC既然是面向连接的，也就像TCP一样，是一个数据流，发送的数据在这个数据流里面有个偏移量offset，可以通过offset查看数据发送到了那里，这样只有这个offset的包没有来，就要重发。如果来了，按照offset拼接，还是能够拼成一个流。

- 机制三： 无阻塞的多路复用

有了自定义的连接和重传机制，就可以解决上面HTTP2.0的多路复用问题

**同HTTP2.0一样，同一条 QUIC连接上可以创建多个stream，来发送多个HTTP请求，但是，QUIC是基于UDP的，一个连接上的多个stream之间没有依赖。**这样，假如stream2丢了一个UDP包，后面跟着stream3的一个UDP包，虽然stream2的那个包需要重新传，但是stream3的包无需等待，就可以发给用户。

- 机制四：自定义流量控制

TCP的流量控制是通过滑动窗口协议。QUIC的流量控制也是通过`window_update`，来告诉对端它可以接受的字节数。**但是QUIC的窗口是适应自己的多路复用机制的，不但在一个连接上控制窗口，还在一个连接中的每个steam控制窗口。**

在TCP协议中，接收端的窗口的起始点是下一个要接收并且ACK的包，即便后来的包都到了，放在缓存里面，窗口也不能右移，因为TCP的ACK机制是基于序列号的累计应答，一旦ACK了一个序列号，就说明前面的都到了，所以是要前面的没到，后面的到了也不能ACK,就会导致后面的到了，也有可能超时重传，浪费带宽

QUIC的ACK是基于offset的，每个offset的包来了，进了缓存，就可以应答，应答后就不会重发，中间的空档会等待到来或者重发，而窗口的起始位置为当前收到的最大offset，从这个offset到当前的stream所能容纳的最大缓存，是真正的窗口的大小，显然，那样更加准确。



### 状态码

https://blog.csdn.net/laishaohe/article/details/79052085





## 单点登录机制

单点登录在现在的系统架构中广泛存在，他将多个子系统的认证体系打通，实现了一个入口多处使用

### 单机系统

#### http无状态协议

任何用户都能通过浏览器访问服务器资源，如果想保护服务器的某些资源，必须限制浏览器请求；要限制浏览器请求，必须鉴别浏览器请求，响应合法请求，忽略非法请求；要鉴别浏览器请求，必须清楚浏览器请求状态。既然http协议无状态，那就让服务器和浏览器共同维护一个状态吧！这就是会话机制。	

#### 会话cookie机制

将会话id作为每一个请求的参数，服务器接收请求自然能解析参数获得会话id，并借此判断是否来自同一会话，很明显，这种方式不靠谱。那就浏览器自己来维护这个会话id吧，每次发送http请求时浏览器自动发送会话id，cookie机制正好用来做这件事。cookie是浏览器用来存储少量数据的一种机制，**数据以”key/value“形式存储**，浏览器发送http请求时自动附带cookie信息

tomcat会话机制当然也实现了cookie，访问tomcat服务器时，浏览器中可以看到一个名为“`JSESSIONID`”的`cookie`，这就是tomcat会话机制维护的会话id，使用了cookie的请求响应过程如下图

![518293d9-64b2-459c-9d45-9f353c757d1f](https://images2015.cnblogs.com/blog/797930/201611/797930-20161129155234443-99011212.png)

#### session

![70e396fa-1bf2-42f8-a504-ce20306e31fa](https://images2015.cnblogs.com/blog/797930/201611/797930-20161129155235693-1708276896.png)





### [如何防止CSRF攻击？](https://tech.meituan.com/2018/10/11/fe-security-csrf.html)

### CSRF的特点

- 攻击一般发起在第三方网站，而不是被攻击的网站。被攻击的网站无法防止攻击发生。
- 攻击利用受害者在被攻击网站的登录凭证，冒充受害者提交操作；而不是直接窃取数据。
- 整个过程攻击者并不能获取到受害者的登录凭证，仅仅是“冒用”。
- 跨站请求可以用各种方式：图片URL、超链接、CORS、Form提交等等。部分请求方式可以直接嵌入在第三方论坛、文章中，难以进行追踪。

**CSRF的两个特点**：

- CSRF（通常）发生在第三方域名。
- CSRF攻击者不能获取到Cookie等信息，只是使用。

针对这两点，我们可以专门制定防护策略，如下：

- 阻止不明外域的访问

  - 同源检测

    Origin和Referer这两个Header在浏览器发起请求时，大多数情况会自动带上，并且不能由前端自定义内容。 服务器可以通过解析这两个Header中的域名，确定请求的来源域。

  - Samesite Cookie

    

- 提交时要求附加本域才能获取的信息

  - CSRF Token

    Token可以在产生并放于Session之中，然后在每次请求时把Token从Session中拿出，与请求中的Token进行比对，但这种方法的比较麻烦的在于如何把Token以参数的形式加入请求(post和header双重校验)

  - 双重Cookie验证



### **单系统登录机制的问题**

单系统登录解决方案的核心是cookie，cookie携带会话id在浏览器与服务器之间维护会话状态。但cookie是有限制的，这个限制就是cookie的域（通常对应网站的域名），**浏览器发送http请求时会自动携带与该域匹配的cookie，而不是所有cookie**



### 多系统

#### 共享session

这个架构我使用了基于Redis的Session共享方案。将Session存储于Redis上，然后将整个系统的全局Cookie Domain设置于[顶级域名](https://www.baidu.com/s?wd=顶级域名&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)上，这样SessionID就能在各个子系统间共享。

这个方案存在着严重的扩展性问题，首先，ASP.NET的Session存储必须为SessionStateItemCollection对象，而存储的结构是经过序列化后经过加密存储的。并且当用户访问应用时，他首先做的就是将存储容器里的所有内容全部取出，并且反序列化为SessionStateItemCollection对象。这就决定了他具有以下约束：

1、  Session中所涉及的类型必须是子系统中共同拥有的（即程序集、类型都需要一致），这导致Session的使用受到诸多限制；

2、  跨[顶级域名](https://www.baidu.com/s?wd=顶级域名&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)的情况完全无法处理；



#### 基于OpenId的单点登录

![img](https://img1.sycdn.imooc.com/5cb53f800001251104490315.jpg)

由上图可以看到，这套单点登录依赖于OpenId的传递，其验证的基础在于OpenId的存储以及发送。

　　　1、当用户第一次登录时，将用户名密码发送给验证服务；

　　　2、验证服务将[用户标识](https://www.baidu.com/s?wd=用户标识&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)OpenId返回到客户端；

　　  3、客户端进行存储；

　　　4、访问子系统时，将OpenId发送到子系统；

　　　5、子系统将OpenId转发到验证服务；

　　　6、验证服务将用户认证信息返回给子系统；

　　　7、子系统构建用户验证信息后将授权后的内容返回给客户端。

这套单点登录验证机制的主要问题在于他基于[C/S架构](https://www.baidu.com/s?wd=C%2FS架构&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)下将用户的OpenId存储于客户端，在子系统之间发送OpenId，而[B/S模式](https://www.baidu.com/s?wd=B%2FS模式&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)下要做到这一点就显得较为困难。为了处理这个问题我们将引出下一种方式，这种方式将解决[B/S模式](https://www.baidu.com/s?wd=B%2FS模式&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)下的OpenId的存储、传递问题。

#### 基于Cookie的OpenId存储方案



#### JWT





### SSO单点退出的方案

1. 其中一个站点退出登录时，仅清除本地凭证和远程验证中心凭证，所有站点每次跳转时，访问远程验证中心，检测其他站点是否已退出登录，如果是，则清除本地凭证，并返回登录页面。（每次跳转都要访问远程验证中心，性能较低，胜在绝对不会有漏洞）
2. 其中一个站点退出登录时，清除本地凭证和远程验证中心凭证，同时按事先设定好的顺序，依次执行各站点提供的清除本地凭证的链接，最后一个站点的清除本地凭证链接中，清除本地凭证后，跳转到登录页面（退出登录时，相当于要跳转一遍所有站点，退出比较慢，而且有些站点其实并没有登录过，白白浪费请求时间，而且如果是java那么因为通过接口调用“各站点提供的清除本地凭证的链接”所以不会获取到浏览器里面request就没办法清楚session、cookie）

### 大体架构

![4d58ccfa-0114-486d-bec2-c28f2f9eb513](https://images2015.cnblogs.com/blog/797930/201611/797930-20161129155238881-1171826792.png)

**1、登录**

相比于单系统登录，sso需要一个独立的**认证中心**，只有认证中心能接受用户的用户名密码等安全信息，其他系统不提供登录入口，只接受认证中心的间接授权。间接授权通过令牌实现，sso认证中心验证用户的用户名密码没问题，创建授权令牌，在接下来的跳转过程中，**授权令牌**作为参数发送给各个子系统，子系统拿到令牌，即得到了授权，可以借此创建局部会话，局部会话登录方式与单系统的登录方式相同。这个过程，也就是单点登录的原理，用下图说明

![img](https://images2015.cnblogs.com/blog/797930/201612/797930-20161203152650974-276822362.png)

用户登录成功之后，会与sso认证中心及各个子系统建立会话，用户与sso认证中心建立的会话称为全局会话，用户与各个子系统建立的会话称为局部会话，局部会话建立之后，用户访问子系统受保护资源将不再通过sso认证中心，全局会话与局部会话有如下约束关系

1. 局部会话存在，全局会话一定存在
2. 全局会话存在，局部会话不一定存在
3. 全局会话销毁，局部会话必须销毁

**2、注销**

　　单点登录自然也要单点注销，在一个子系统中注销，所有子系统的会话都将被销毁，用下面的图来说明![3b139d2e-0b83-4a69-b4f2-316adb8997ce](https://images2015.cnblogs.com/blog/797930/201611/797930-20161129155243068-1378377736.png)

[单点登录原理与简单实现](https://www.cnblogs.com/ywlaker/p/6113927.html)



## 网络安全

### SYN 攻击

在三次握手过程中，服务器发送 SYN-ACK 之后，收到客户端的 ACK 之前的 TCP 连接称为半连接(half-open connect)。此时服务器处于 SYN_RCVD 状态。当收到 ACK 后，服务器才能转入 ESTABLISHED 状态.

- SYN 攻击指的是，攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送SYN包，服务器回复确认包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，导致目标系统运行缓慢，严重者会引起网络堵塞甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。
- 检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击。在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击。
- SYN攻击不能完全被阻止，除非将TCP协议重新设计。我们所做的是尽可能的减轻SYN攻击的危害，常见的防御 SYN 攻击的方法有如下几种：
- 缩短超时时间: 增加最大半连接数，过滤网关防护，SYN cookies技术



DDos

+ SYN DDos
+ UDP\ICMP DDos

单包攻击

+ 畸形报文攻击
  + smurf
  + Fraggle
  + IP Snooping
  + Landing
  + Ping of Death
+ 扫描类攻击
  + IP地址扫描
  + 端口扫描
+ 特殊控制类攻击
  + 超大ICMP报文
  + Tracert和时间戳选项IP报文控制

## TCP粘包

**什么是粘包现象**

TCP粘包是指发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。

**拆包：**一段数据因为发送窗口被拆成多个包

![img](https://pic1.zhimg.com/80/v2-c7d4903f383c026b47e09f2f8a8a7220_720w.jpg)

### 为什么出现粘包现象

**发送方原因**

我们知道，TCP默认会使用Nagle算法。而Nagle算法主要做两件事：**1）只有上一个分组得到确认，才会发送下一个分组；2）收集多个小分组，在一个确认到来时一起发送。**

所以，正是Nagle算法造成了发送方有可能造成粘包现象。

**接收方原因**

TCP接收到分组时，并不会立刻送至应用层处理，或者说，应用层并不一定会立即处理；实际上，TCP将收到的分组保存至接收缓存里，然后应用程序主动从缓存里读收到的分组。这样一来，**如果TCP接收分组的速度大于应用程序读分组的速度，多个包就会被存至缓存，应用程序读时，就会读到多个首尾相接粘到一起的包。**



### **什么时候需要处理粘包现象**

（1）如果发送方发送的多个分组本来就是同一个数据的不同部分，比如一个很大的文件被分成多个分组发送，这时，当然不需要处理粘包的现象；

（2）但如果多个分组本毫不相干，甚至是并列的关系，我们就一定要处理粘包问题了。

### 如何处理粘包现象

（1）发送方: 对于发送方造成的粘包现象，我们可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭Nagle算法。

（2）接收方: 遗憾的是TCP并没有处理接收方粘包现象的机制，我们只能在应用层进行处理。

（3）应用层处理

应用层的处理简单易行！并且不仅可以解决接收方造成的粘包问题，还能解决发送方造成的粘包问题。

解决方法就是**循环处理**：应用程序在处理从缓存读来的分组时，读完一条数据时，就应该循环读下一条数据，直到所有的数据都被处理；但是如何判断每条数据的长度呢？

两种途径：

1）**格式化数据**：每条数据有固定的格式（开始符、结束符），这种方法简单易行，但选择开始符和结束符的时候一定要注意每条数据的内部一定不能出现开始符或结束符；

2）**发送长度**：发送每条数据的时候，将数据的长度一并发送，比如可以选择每条数据的前4字节是数据的长度（一个int来储存数据长度大小），应用层处理时可以根据长度来判断每条数据的开始和结束。

```
1. 使用带消息头的协议、消息头存储消息开始标识及消息长度信息，服务端获取消息头的时候解析出消息长度，然后向后读取该长度的内容。
2. 设置定长消息，服务端每次读取既定长度的内容作为一条完整消息，当消息不够长时，空位补上固定字符。
3. 设置消息边界，服务端从网络流中按消息编辑分离出消息内容，一般使用‘\n’。
4. 更为复杂的协议，例如楼主最近接触比较多的车联网协议808,809协议。
```



## TCP head

TCP头部的最后一个选项字段（options）是可变长的可选信息。这部分最多包含40字节，因为TCP头部最长是60字节（其中还包含前面讨论的20字节的固定部分）。

选项的第一个字段kind说明选项的类型。有的TCP选项没有后面两个字段，仅包含1字节的kind字段。第二个字段length（如果有的话）指定该选项的总长度，该长度包括kind字段和length字段占据的2字节。第三个字段info（如果有的话）是选项的具体信息。常见的TCP选项有7种。

+ kind=0是选项表结束选项。

+ kind=1是空操作（nop）选项，没有特殊含义，一般用于将TCP选项的总长度填充为4字节的整数倍。

+ kind=2是最大报文段长度选项。TCP连接初始化时，通信双方使用该选项来协商最大报文段长度（Max Segment Size，MSS）。TCP模块通常将MSS设置为（MTU-40）字节（减掉的这40字节包括20字节的TCP头部和20字节的IP头部）。这样携带TCP报文段的IP数据报的长度就不会超过MTU（假设TCP头部和IP头部都不包含选项字段，并且这也是一般情况），从而避免本机发生IP分片。对以太网而言，MSS值是1460（1500-40）字节。

+ kind=3是窗口扩大因子选项。TCP连接初始化时，通信双方使用该选项来协商接收通告窗口的扩大因子。在TCP的头部中，接收通告窗口大小是用16位表示的，故最大为65535字节，但实际上TCP模块允许的接收通告窗口大小远不止这个数（为了提高TCP通信的吞吐量）。窗口扩大因子解决了这个问题。假设TCP头部中的接收通告窗口大小是N，窗口扩大因子（移位数）是M，那么TCP报文段的实际接收通告窗口大小是N乘2M，或者说N左移M位。注意，M的取值范围是0～14。我们可以通过修改/proc/sys/net/ipv4/tcp_window_scaling内核变量来启用或关闭窗口扩大因子选项。

  和MSS选项一样，窗口扩大因子选项只能出现在同步报文段中，否则将被忽略。但同步报文段本身不执行窗口扩大操作，即同步报文段头部的接收通告窗口大小就是该TCP报文段的实际接收通告窗口大小。当连接建立好之后，每个数据传输方向的窗口扩大因子就固定不变了。关于窗口扩大因子选项的细节，可参考标准文档RFC 1323。

+ kind=4是选择性确认（Selective Acknowledgment，SACK）选项。TCP通信时，如果某个TCP报文段丢失，则TCP模块会重传最后被确认的TCP报文段后续的所有报文段，这样原先已经正确传输的TCP报文段也可能重复发送，从而降低了TCP性能。SACK技术正是为改善这种情况而产生的，它使TCP模块只重新发送丢失的TCP报文段，不用发送所有未被确认的TCP报文段。选择性确认选项用在连接初始化时，表示是否支持SACK技术。我们可以通过修改/proc/sys/net/ipv4/tcp_sack内核变量来启用或关闭选择性确认选项。

+ kind=5是SACK实际工作的选项。该选项的参数告诉发送方本端已经收到并缓存的不连续的数据块，从而让发送端可以据此检查并重发丢失的数据块。每个块边沿（edge of block）参数包含一个4字节的序号。其中块左边沿表示不连续块的第一个数据的序号，而块右边沿则表示不连续块的最后一个数据的序号的下一个序号。这样一对参数（块左边沿和块右边沿）之间的数据是没有收到的。因为一个块信息占用8字节，所以TCP头部选项中实际上最多可以包含4个这样的不连续数据块（考虑选项类型和长度占用的2字节）。

+ kind=8是时间戳选项。该选项提供了较为准确的计算通信双方之间的回路时间（Round Trip Time，RTT）的方法，从而为TCP流量控制提供重要信息。我们可以通过修改/proc/sys/net/ipv4/tcp_timestamps内核变量来启用或关闭时间戳选项。



## HTTPS

加密过程：

1. Hello - 握手开始于客户端发送Hello消息。包含服务端为了通过SSL连接到客户端的所有信息，包括客户端支持的各种密码套件和最大SSL版本。服务器也返回一个Hello消息，包含客户端需要的类似信息，包括到底使用哪一个加密算法和SSL版本。
2. 证书交换 - 现在连接建立起来了，服务器必须通过SSL证书证明他的身份。SSL证书包含各种数据，包含所有者名称，相关属性（域名），证书上的公钥，数字签名和关于证书有效期的信息。客户端检查它是不是被CA验证过的且根据数字签名验证内容是否被修改过。注意服务器被允许需求一个证书去证明客户端的身份，但是这个只发生在敏感应用。
3. 密钥交换 - 使用RSA非对称公钥加密算法（客户端生成一个对称密钥，然后用SSL证书里带的服务器公钥将该对称密钥加密。随后发送到服务端，服务端用服务器私钥解密，到此，握手阶段完成。）或者DH交换算法在客户端与服务端双方确定将要使用的密钥。这个密钥是双方都同意的一个简单，对称的密钥。这个过程是基于非对称加密方式和服务器的公钥/私钥的。
4. 加密通信 - 在服务器和客户端加密实际信息是用到对称加密算法，用哪个算法在Hello阶段已经确定。对称加密算法是对于加密和解密都很简单的密钥。这个密钥是基于第三步在客户端与服务端已经商议好的。与需要公钥/私钥的非对称加密算法相反。

![此图并不准确](https://leran2deeplearnjavawebtech.oss-cn-beijing.aliyuncs.com/somephoto/tls%E6%B5%81%E7%A8%8B.png)



### 为什么使用混合加密

### 共享密钥加密

加密和解密公用一套秘钥，这样就会产生问题，已共享秘钥加密方式必须将秘钥传送给对方，但如果通信被监听，那么秘钥可能会被泄漏产生危险。

### 公开秘钥加密

公开秘钥加密使用一种非对称加密的算法，使用一对非对称的秘钥，一把叫做共有秘钥，一把叫做私有秘钥，在加密的时候，通信的一方使用共有秘钥进行加密，通信的另一方使用私有秘钥进行解密，利用这种方式不需要发送私有秘钥，也就不存在泄漏的风险了。

### https加密方式

因为公开秘钥加密的方式比共享秘钥加密的方式钥消耗cpu资源，https采取了混合加密的方式，来结合两者的优点。

在秘钥交换阶段使用公开加密的方式，之后建立连接后使用共享秘钥加密方式进行加密

## 跨域问题

[跨域问题解决方案](https://juejin.im/post/5dc9228cf265da4d2125de8d#heading-12)

跨域是指一个域下的文档或脚本试图去请求另一个域下的资源，这里跨域是广义的。

**什么是同源策略？**
同源策略/SOP（Same origin policy）是一种约定，由Netscape公司1995年引入浏览器，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，浏览器很容易受到XSS、CSFR等攻击。**所谓同源是指"协议+域名+端口"三者相同，即便两个不同的域名指向同一个ip地址，也非同源。**

同源策略限制以下几种行为：

```
1.) Cookie、LocalStorage 和 IndexDB 无法读取
2.) DOM 和 Js对象无法获得
3.) AJAX 请求不能发送
```

### **跨域解决方案**

**1、 通过jsonp跨域·**

**2、 document.domain + iframe跨域**

**3、 location.hash + iframe**

**4、 window.name + iframe跨域**

**5、 postMessage跨域**

**6、 跨域资源共享（CORS）**

在CORS中，所有的跨域请求被分为了两种类型，一种是**简单请求**，一种是复杂请求 (严格来说应该叫‘需预检请求’)；简单请求与普通的ajax请求无异；但**复杂请求**，必须在正式发送请求前先发送一个OPTIONS方法的请求已得到服务器的同意，若没有得到服务器的同意，浏览器不会发送正式请求；

上面的请求其实就是个简单请求类型；那么简单类型和复杂类型是如何区分的呢？

满足以下所有条件，被视为简单类型的请求：

1. 请求方法必须是 GET、HEAD、POST中的一种，其他方法不行；

2. 请求头类型只能是 Accept、Accept-Language、Content-Language、Content-Type，添加其他额外请求头不行；

3. 请求头 Content-Type 如果有，值只能是 text/plain、multipart/form-data、application/x-www-form-urlencoded 中的一种，其他值不行；

4. 请求中的任意 XMLHttpRequestUpload 对象均没有注册任何事件监听器；

5. 请求中没有使用 ReadableStream 对象。

而以上的条件有任意一条不满足，则视为复杂类型的请求；前面说过，如果是复杂请求，浏览器会先发送OPTIONS方法的请求以取得服务器的确认

![img](https://pic4.zhimg.com/80/v2-450c2b4db198c186356090f755fde4f3_720w.jpg)

要解决这个问题，我们就需要在服务器添加对应的响应头信息

![img](https://pic4.zhimg.com/80/v2-241a00c721b4a54cee5e93f85d4923cf_720w.jpg)



`Access-Control-Allow-Origin` ： 指示请求的资源能共享给哪些域，可以是具体的域名或者*表示所有域。

`Access-Control-Allow-Credentials`： 指示当请求的凭证标记为 true 时，是否响应该请求。

`Access-Control-Allow-Headers`: 用在对预请求的响应中，指示实际的请求中可以使用哪些 HTTP 头。

`Access-Control-Allow-Methods`： 指定对预请求的响应中，哪些 HTTP 方法允许访问请求的资源。

`Access-Control-Expose-Headers` ： 指示哪些 HTTP 头的名称能在响应中列出。

`Access-Control-Max-Age `： 指示预请求的结果能被缓存多久。

`Access-Control-Request-Headers `：用于发起一个预请求，告知服务器正式请求会使用那些 HTTP 头。

`Access-Control-Request-Method`： 用于发起一个预请求，告知服务器正式请求会使用哪一种 HTTP 请求方法。

`Origin `： 指示获取资源的请求是从什么域发起的





**示例**

```java
@Configuration
public class CORSConfiguration {
    @Bean
    public WebMvcConfigurer corsConfigurer() {
        return new WebMvcConfigurer() {
            @Override
            public void addCorsMappings(CorsRegistry registry) {
                registry.addMapping("/prize/**")
                        .allowedOrigins("*")
                        .allowedHeaders("*")
                        .allowCredentials(true)
                        .allowedMethods("GET", "POST", "DELETE", "PUT","PATCH")
                        .maxAge(3600);
            }
        };
    }
}
```

**7、 nginx代理跨域**

浏览器跨域访问js、css、img等常规静态资源被同源策略许可，但iconfont字体文件(eot|otf|ttf|woff|svg)例外，此时可在nginx的静态资源服务器中加入以下配置。

```nginx
location / {
  add_header Access-Control-Allow-Origin *;
}
```

##### **nginx反向代理接口跨域**

跨域原理： 同源策略是浏览器的安全策略，不是HTTP协议的一部分。服务器端调用HTTP接口只是使用HTTP协议，不会执行JS脚本，不需要同源策略，也就不存在跨越问题。

实现思路：通过nginx配置一个代理服务器（域名与domain1相同，端口不同）做跳板机，反向代理访问domain2接口，并且可以顺便修改cookie中domain信息，方便当前域cookie写入，实现跨域登录。

nginx具体配置：                 

```nginx
#proxy服务器
server {
    listen       81;
    server_name  www.domain1.com;

    location / {
        proxy_pass   http://www.domain2.com:8080;  #反向代理
        proxy_cookie_domain www.domain2.com www.domain1.com; #修改cookie里域名
        index  index.html index.htm;

        # 当用webpack-dev-server等中间件代理接口访问nignx时，此时无浏览器参与，故没有同源限制，下面的跨域配置可不启用
        add_header Access-Control-Allow-Origin http://www.domain1.com;  #当前端只跨域不带cookie时，可为*
        add_header Access-Control-Allow-Credentials true;
    }
}
```

**8、 nodejs中间件代理跨域**

**9、 WebSocket协议跨域**







## GET和POST的区别？

> 就下面的找几个点和面试官侃侃而谈即可，不可能全部都记得，想到什么讲什么吧

- GET 被强制服务器支持
- 浏览器对URL的长度有限制，所以GET请求不能代替POST请求发送大量数据
- GET请求发送**数据更小**
- GET请求是**不安全**的
- GET请求是**幂等**的
  - 幂等的意味着对同一URL的多个请求应该返回同样的结果
- POST请求不能被缓存
- POST请求相对GET请求是「安全」的
  - 这里安全的含义仅仅是指是非修改信息
- GET用于信息获取，而且是安全的和幂等的
  - 所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET 请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。
- POST是用于修改服务器上的资源的请求
- 发送包含未知字符的用户输入时，POST 比 GET 更稳定也更可靠

**引申：说完原理性的问题，我们从表面上来看看GET和POST的区别：**

- GET是从服务器上获取数据，POST是向服务器传送数据。 GET和 POST只是一种传递数据的方式，GET也可以把数据传到服务器，**他们的本质都是发送请求和接收结果。**只是组织格式和数据量上面有差别，http协议里面有介绍
- GET是把参数数据队列加到提交表单的ACTION属性所指的URL中，值和表单内各个字段一一对应，在URL中可以看到。POST是通过HTTP POST机制，将表单内各个字段与其内容放置在HTML HEADER内一起传送到ACTION属性所指的URL地址。用户看不到这个过程。 因为GET设计成传输小数据，而且最好是不修改服务器的数据，所以浏览器一般都在地址栏里面可以看到，但POST一般都用来传递大数据，或比较隐私的数据，所以在地址栏看不到，能不能看到不是协议规定，是浏览器规定的。
- 对于GET方式，服务器端用Request.QueryString获取变量的值，对于POST方式，服务器端用Request.Form获取提交的数据。 没明白，怎么获得变量和你的服务器有关，和GET或POST无关，服务器都对这些请求做了封装
- GET传送的数据量较小，不能大于2KB。POST传送的数据量较大，一般被默认为不受限制。但理论上，IIS4中最大量为80KB，IIS5中为100KB。 POST基本没有限制，我想大家都上传过文件，都是用POST方式的。只不过要修改form里面的那个type参数
- GET安全性非常低，POST安全性较高。 如果没有加密，他们安全级别都是一样的，随便一个监听器都可以把所有的数据监听到。









## 前端

#### 1. 浏览器渲染机制

- 浏览器采用流式布局模型（`Flow Based Layout`）
- 浏览器会把`HTML`解析成`DOM`，把`CSS`解析成`CSSOM`，`DOM`和`CSSOM`合并就产生了渲染树（`Render Tree`）。
- 有了`RenderTree`，我们就知道了所有节点的样式，然后计算他们在页面上的大小和位置，最后把节点绘制到页面上。
- 由于浏览器使用流式布局，对`Render Tree`的计算通常只需要遍历一次就可以完成，**但`table`及其内部元素除外，他们可能需要多次计算，通常要花3倍于同等元素的时间，这也是为什么要避免使用`table`布局的原因之一**。

#### 2. 重绘

由于节点的几何属性发生改变或者由于样式发生改变而不会影响布局的，称为重绘，例如`outline`, `visibility`, `color`、`background-color`等，重绘的代价是高昂的，因为浏览器必须验证DOM树上其他节点元素的可见性。

#### 3. 回流

回流是布局或者几何属性需要改变就称为回流。回流是影响浏览器性能的关键因素，因为其变化涉及到部分页面（或是整个页面）的布局更新。一个元素的回流可能会导致了其所有子元素以及DOM中紧随其后的节点、祖先节点元素的随后的回流。

我们前面知道了，回流这一阶段主要是计算节点的位置和几何信息，那么当页面布局和几何信息发生变化的时候，就需要回流。比如以下情况：

- 添加或删除可见的DOM元素
- 元素的位置发生变化
- 元素的尺寸发生变化（包括外边距、内边框、边框大小、高度和宽度等）
- 内容发生变化，比如文本变化或图片被另一个不同尺寸的图片所替代。
- 页面一开始渲染的时候（这肯定避免不了）
- 浏览器的窗口尺寸变化（因为回流是根据视口的大小来计算元素的位置和大小的）

**回流必定会发生重绘，重绘不一定会引发回流。**



## HTTP协议

有三种常见的中介形式:代理、网关和隧道。如果不是tunnel，最好使用cache减少请求

### 2.惯例语法

